#! /usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLM æœåŠ¡æ¨¡å—
æ”¯æŒå¤šç§å¤§è¯­è¨€æ¨¡åž‹ï¼ˆOpenAIã€Claudeã€DeepSeekï¼‰
æä¾›å¯¹è¯ç®¡ç†ã€å›¾åƒåˆ†æžç­‰åŠŸèƒ½
"""
import json
import logging
import time
from datetime import datetime

import litellm
from litellm import Router

from true_love_ai.configuration import Config
from true_love_ai.llm.constants import (
    OPENAI_MODEL, OPENAI_VISION_MODEL, CLAUDE_MODEL, DEEPSEEK_MODEL,
    DEFAULT_MODEL, MAX_CONVERSATION_LENGTH
)
from true_love_ai.llm.function_calls import TYPE_ANSWER_CALL, IMG_TYPE_ANSWER_CALL
from true_love_ai.services.search_service import fetch_baidu_references
from true_love_ai.services.image_service import ImageService

# æ¨¡å—åç§°ï¼ˆç”¨äºŽé…ç½®è¯†åˆ«ï¼‰
name = "chatgpt"

# LiteLLM é…ç½®
litellm.modify_params = True
litellm.drop_params = True


def fetch_stream(ret, is_function_call: bool = False) -> str:
    """
    å¤„ç†æµå¼å“åº”
    
    Args:
        ret: æµå¼å“åº”å¯¹è±¡
        is_function_call: æ˜¯å¦ä¸º function call å“åº”
        
    Returns:
        æ‹¼æŽ¥åŽçš„å“åº”å­—ç¬¦ä¸²
    """
    rsp = ''
    for stream_res in ret:
        try:
            if is_function_call:
                # å¤„ç†å‡½æ•°/å·¥å…·è°ƒç”¨
                if stream_res.choices[0].delta.tool_calls:
                    tool_call = stream_res.choices[0].delta.tool_calls[0]
                    if tool_call.function.arguments:
                        rsp += tool_call.function.arguments.replace('\n\n', '\n')
            else:
                # å¤„ç†æ™®é€šæ–‡æœ¬å†…å®¹
                if stream_res.choices[0].delta.content:
                    rsp += stream_res.choices[0].delta.content.replace('\n\n', '\n')
        except Exception as e:
            logging.debug(f"å¤„ç†æµå¼å“åº”æ—¶å‡ºé”™: {e}")
            continue
    return rsp


class LLMService:
    """
    å¤§è¯­è¨€æ¨¡åž‹æœåŠ¡
    æ”¯æŒ OpenAIã€Claudeã€DeepSeek ç­‰å¤šç§æ¨¡åž‹
    """

    def __init__(self) -> None:
        self.LOG = logging.getLogger("LLMService")
        self.config = Config().LLM_BOT
        self.current_model = DEFAULT_MODEL
        
        # åˆå§‹åŒ–è·¯ç”±å™¨ï¼ˆæ”¯æŒå¤š API Key è´Ÿè½½å‡è¡¡ï¼‰
        self.router = self._init_router()
        
        # å¯¹è¯åŽ†å²å®¹å™¨
        self.conversation_list = {}
        
        # åŠ è½½ç³»ç»Ÿæç¤ºè¯
        self._load_prompts()
        
        # å›¾åƒæœåŠ¡
        self.image_service = ImageService()

    def _init_router(self) -> Router:
        """åˆå§‹åŒ– LiteLLM è·¯ç”±å™¨"""
        model_list = [
            # OpenAI å¤š Key é…ç½® (GPT-5 ä¸æ”¯æŒè‡ªå®šä¹‰ temperatureï¼Œåªèƒ½ç”¨ temperature=1)
            {"model_name": OPENAI_MODEL, "litellm_params": {"model": OPENAI_MODEL, "api_key": self.config.get('key1'), "temperature": 1}},
            {"model_name": OPENAI_MODEL, "litellm_params": {"model": OPENAI_MODEL, "api_key": self.config.get('key2'), "temperature": 1}},
            {"model_name": OPENAI_MODEL, "litellm_params": {"model": OPENAI_MODEL, "api_key": self.config.get('key3'), "temperature": 1}},
            # Claude
            {"model_name": CLAUDE_MODEL, "litellm_params": {"model": CLAUDE_MODEL, "api_key": self.config.get('claude_key1')}},
            # DeepSeek
            {"model_name": DEEPSEEK_MODEL, "litellm_params": {"model": DEEPSEEK_MODEL, "api_key": self.config.get('ds_key1')}},
        ]
        return Router(model_list=model_list)

    def _load_prompts(self) -> None:
        """åŠ è½½ç³»ç»Ÿæç¤ºè¯"""
        self.prompts = {
            'default': {"role": "system", "content": self.config.get("prompt", "")},
            'gpt4': {"role": "system", "content": self.config.get("prompt2", "")},
            'xunwen': {"role": "system", "content": self.config.get("prompt3", "")},
            'img_prompt': {"role": "system", "content": self.config.get("prompt4", "")},
            'img_type': {"role": "system", "content": self.config.get("prompt5", "")},
            'img_analyze': {"role": "system", "content": self.config.get("prompt6", "")},
        }

    # ==================== æ ¸å¿ƒå¯¹è¯æ–¹æ³• ====================
    
    def get_answer(self, question: str, wxid: str, sender: str) -> dict:
        """
        èŽ·å–èŠå¤©å›žç­”ï¼ˆä¸»å…¥å£ï¼‰
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            wxid: ç”¨æˆ·æ ‡è¯†
            sender: å‘é€è€…
            
        Returns:
            {"type": str, "answer": str, "debug": str(å¯é€‰)}
        """
        clean_question = question.replace("debug", "", 1) if question else 'ä½ å¥½'
        self._update_message(wxid, clean_question, "user")
        
        start_time = time.time()
        self.LOG.info(f"å¼€å§‹è°ƒç”¨ LLM, model: {self.current_model}")
        
        rsp = self._send_chat_request(self.current_model, wxid)
        
        cost = round(time.time() - start_time, 2)
        self.LOG.info(f"å›žç­”è€—æ—¶: {cost}s")
        
        if question.startswith('debug'):
            rsp['debug'] = f"(aiCost: {cost}s, model: {self.current_model})"
        
        return rsp

    def _send_chat_request(self, model: str, wxid: str) -> dict:
        """
        å‘é€èŠå¤©è¯·æ±‚
        
        Args:
            model: ä½¿ç”¨çš„æ¨¡åž‹
            wxid: ç”¨æˆ·æ ‡è¯†
            
        Returns:
            {"type": str, "answer": str}
        """
        try:
            question = self.conversation_list[wxid][-1]
            
            # å…ˆåˆ¤æ–­æ¶ˆæ¯ç±»åž‹
            ret = self.router.completion(
                model=model,
                messages=self.conversation_list[wxid],
                tool_choice={"type": "function", "function": {"name": "type_answer"}},
                tools=TYPE_ANSWER_CALL,
                stream=True
            )
            
            rsp_str = fetch_stream(ret, True)
            result = json.loads(rsp_str)
            self.LOG.info(f"LLM ç±»åž‹åˆ¤æ–­ç»“æžœ: {result}")
            
            # å¦‚æžœéœ€è¦æœç´¢ï¼Œæ‰§è¡Œæœç´¢å¢žå¼º
            if result['type'] == 'search':
                result = self._handle_search_request(model, wxid, question, result)
            
            self._update_message(wxid, rsp_str, "assistant")
            return result
            
        except Exception as e:
            self.LOG.exception(f"è°ƒç”¨ LLM æœåŠ¡å‡ºé”™: {e}")
            return {"type": "chat", "answer": "å‘ç”ŸæœªçŸ¥é”™è¯¯, ç¨åŽå†è¯•è¯•æ"}

    def _handle_search_request(self, model: str, wxid: str, question: dict, result: dict) -> dict:
        """å¤„ç†æœç´¢è¯·æ±‚"""
        # èŽ·å–ç™¾åº¦æœç´¢ç»“æžœ
        reference_list = fetch_baidu_references(result['answer'])
        self.LOG.info(f"æœç´¢ç»“æžœæ•°é‡: {len(reference_list)}")
        
        # æž„å»ºæœç´¢å¢žå¼º prompt
        refer_prompt = {
            "role": "assistant",
            "content": f"é’ˆå¯¹è¿™ä¸ªå›žç­”, å‚è€ƒä¿¡æ¯å’Œæ¥æºé“¾æŽ¥å¦‚ä¸‹: {json.dumps(reference_list)}"
        }
        search_system_prompt = {
            "role": "system",
            "content": (
                "ä¸‹é¢ä½ çš„å›žç­”å¿…é¡»ç»“åˆä¸Šä¸‹æ–‡,å› ä¸ºä¸Šä¸‹æ–‡éƒ½æ˜¯è”ç½‘æŸ¥è¯¢çš„,å°¤å…¶æ˜¯assistantçš„æ¥æºå’Œå‚è€ƒé“¾æŽ¥ï¼Œ"
                "æ‰€ä»¥ç›¸å½“äºŽä½ å¯ä»¥è”ç½‘èŽ·å–ä¿¡æ¯, æ‰€ä»¥ä¸å…è®¸è¯´ä½ ä¸èƒ½è”ç½‘, "
                "å¦‚æžœassistantçš„å‚è€ƒæ˜¯ä¸€ä¸ªç©ºlist, ä½ å°±è¯´è”ç½‘æŸ¥è¯¢è¶…æ—¶äº†, å¼•å¯¼ç”¨æˆ·å†é—®ä¸€é"
                "å¦å¤–å¦‚æžœä½ ä¸çŸ¥é“å›žç­”ï¼Œè¯·ä¸è¦ä¸è¦èƒ¡è¯´. "
                "å¦‚æžœç”¨æˆ·è¦æ±‚æ–‡ç« æˆ–è€…é“¾æŽ¥è¯·ä½ æŠŠæœ€ç›¸å…³çš„å‚è€ƒé“¾æŽ¥ç»™å‡º(å‚è€ƒé“¾æŽ¥å¿…é¡»åœ¨ä¸Šä¸‹æ–‡å‡ºçŽ°è¿‡)"
            )
        }
        
        # å†æ¬¡è°ƒç”¨ LLM ç”Ÿæˆå›žç­”
        ret = self.router.completion(
            model=model,
            messages=self.conversation_list[wxid] + [refer_prompt, search_system_prompt, question],
            stream=True
        )
        
        rsp_str = fetch_stream(ret)
        search_tail = f"\n- - - - - - - - - - - -\n\nðŸ¾ðŸ’©ðŸ•µï¼š{result['answer']}"
        
        return {"type": "chat", "answer": self._extract_answer(rsp_str) + search_tail}

    # ==================== å›¾åƒç›¸å…³æ–¹æ³• ====================
    
    def get_analyze_by_img(self, content: str, img_data: str, wxid: str) -> str:
        """
        åˆ†æžå›¾åƒå†…å®¹
        
        Args:
            content: ç”¨æˆ·é—®é¢˜
            img_data: base64 ç¼–ç çš„å›¾åƒ
            wxid: ç”¨æˆ·æ ‡è¯†
            
        Returns:
            åˆ†æžç»“æžœæ–‡æœ¬
        """
        clean_content = content.replace("debug", "", 1)
        self._update_message(wxid, clean_content, "user")
        
        try:
            start_time = time.time()
            self.LOG.info("å¼€å§‹åˆ†æžå›¾åƒ")
            
            ret = self.router.completion(
                model=OPENAI_VISION_MODEL,
                messages=[
                    self.prompts['img_analyze'],
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": content},
                            {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{img_data}"}}
                        ]
                    }
                ],
                stream=True
            )
            
            cost = round(time.time() - start_time, 2)
            self.LOG.info(f"å›¾åƒåˆ†æžè€—æ—¶: {cost}s")
            
            result = fetch_stream(ret)
            self._update_message(wxid, result, "assistant")
            
            if content.startswith('debug'):
                result = f"{result}\n\n(aiCost: {cost}s, model: {OPENAI_VISION_MODEL})"
            
            return result
            
        except Exception:
            self.LOG.exception("å›¾åƒåˆ†æžé”™è¯¯")
            raise

    def get_img_type(self, content: str) -> str:
        """
        åˆ¤æ–­å›¾åƒæ“ä½œç±»åž‹å¹¶ç”Ÿæˆæè¿°è¯
        
        Args:
            content: ç”¨æˆ·æè¿°
            
        Returns:
            JSON å­—ç¬¦ä¸² {"type": str, "answer": str}
        """
        try:
            start_time = time.time()
            self.LOG.info("å¼€å§‹åˆ¤æ–­å›¾åƒæ“ä½œç±»åž‹")
            
            result = self._send_message(
                messages=[self.prompts['img_type'], {"role": "user", "content": content}],
                function_call={"type": "function", "function": {"name": "img_type_answer_call"}},
                functions=IMG_TYPE_ANSWER_CALL,
            )
            
            self.LOG.info(f"ç±»åž‹åˆ¤æ–­è€—æ—¶: {(time.time() - start_time) * 1000:.0f}ms, result: {result}")
            return result
            
        except Exception:
            self.LOG.exception("åˆ¤æ–­å›¾åƒç±»åž‹é”™è¯¯")
            raise

    def get_img(self, content: str) -> dict:
        """
        æ ¹æ®æ–‡å­—æè¿°ç”Ÿæˆå›¾åƒ
        
        Args:
            content: ç”¨æˆ·æè¿°
            
        Returns:
            {"prompt": str, "img": base64_str}
        """
        # å…ˆç”Ÿæˆå›¾åƒæè¿°è¯
        try:
            start_time = time.time()
            self.LOG.info("å¼€å§‹ç”Ÿæˆå›¾åƒæè¿°è¯")
            
            image_prompt = self._send_message(
                messages=[self.prompts['img_prompt'], {"role": "user", "content": content}]
            )
            
            self.LOG.info(f"æè¿°è¯ç”Ÿæˆè€—æ—¶: {(time.time() - start_time) * 1000:.0f}ms")
        except Exception:
            self.LOG.exception("ç”Ÿæˆå›¾åƒæè¿°è¯é”™è¯¯")
            image_prompt = content
        
        # è°ƒç”¨å›¾åƒæœåŠ¡ç”Ÿæˆå›¾åƒ
        return self.image_service.generate_image(image_prompt)

    def get_img_by_img(self, content: dict, img_data: str) -> dict:
        """
        æ ¹æ®å›¾åƒç”Ÿæˆ/ç¼–è¾‘å›¾åƒ
        
        Args:
            content: {"type": str, "answer": str}
            img_data: base64 ç¼–ç çš„åŽŸå›¾
            
        Returns:
            {"prompt": str, "img": base64_str}
        """
        return self.image_service.edit_image(
            img_data=img_data,
            operation_type=content["type"],
            prompt=content["answer"]
        )

    # ==================== ç‰¹æ®ŠåŠŸèƒ½æ–¹æ³• ====================
    
    def get_xun_wen(self, question: str) -> str:
        """
        è¯¢é—®åŠŸèƒ½ï¼ˆç‰¹å®šæ ¼å¼é—®é¢˜ï¼‰
        
        Args:
            question: æ ¼å¼ä¸º "xxx-å®žé™…é—®é¢˜"
            
        Returns:
            å›žç­”æ–‡æœ¬
        """
        content = question.split("-")[1]
        return self._send_message([self.prompts['xunwen'], {"role": "user", "content": content}])

    # ==================== å·¥å…·æ–¹æ³• ====================
    
    def _send_message(self, messages: list, function_call=None, functions=None) -> str:
        """
        å‘é€æ¶ˆæ¯åˆ° LLM
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            function_call: function call é…ç½®
            functions: functions å®šä¹‰
            
        Returns:
            å“åº”å­—ç¬¦ä¸²
        """
        try:
            ret = self.router.completion(
                model=self.current_model,
                messages=messages,
                tool_choice=function_call,
                tools=functions,
                stream=True
            )
            return fetch_stream(ret, functions is not None)
        except Exception as e:
            self.LOG.error(f"å‘é€æ¶ˆæ¯é”™è¯¯: {e}")
            return "An unknown error has occurred. Try again later."

    def _update_message(self, wxid: str, content: str, role: str) -> None:
        """
        æ›´æ–°å¯¹è¯åŽ†å²
        
        Args:
            wxid: ç”¨æˆ·æ ‡è¯†
            content: æ¶ˆæ¯å†…å®¹
            role: è§’è‰²ï¼ˆuser/assistantï¼‰
        """
        time_mk = (
            f"å½“éœ€è¦å›žç­”å½“å‰æ—¶é—´æˆ–è€…å…³äºŽå½“å‰æ—¥æœŸç±»é—®é¢˜, è¯·ç›´æŽ¥å‚è€ƒè¿™ä¸ªæ—¶é—´: "
            f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
            f"(è¯·æ³¨æ„è¿™æ˜¯ç¾Žå›½ä¸­éƒ¨æ—¶é—´, ä½ å¯ä»¥å‘Šè¯‰åˆ«äººä½ ä½¿ç”¨çš„æ—¶åŒº), "
            f"å¦å¤–ç”¨æˆ·æå‡æ˜¯å¦å¯ä»¥è”ç½‘ä½ éœ€è¦è¯´æˆ‘å·²ç»æŽ¥å…¥è°·æ­Œæœç´¢, "
            f"å¹¶ä¸”çŸ¥è¯†åº“æœ€æ–°æ¶ˆæ¯æ˜¯: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
        )
        
        # åˆå§‹åŒ–å¯¹è¯åŽ†å²
        if wxid not in self.conversation_list:
            # æ ¹æ®ç”¨æˆ·é€‰æ‹©ä¸åŒçš„ç³»ç»Ÿ prompt
            system_prompt = (
                self.prompts['gpt4'] 
                if wxid in self.config.get("gpt4", []) 
                else self.prompts['default']
            )
            self.conversation_list[wxid] = [
                system_prompt,
                {"role": "system", "content": time_mk}
            ]
        
        # æ·»åŠ å½“å‰æ¶ˆæ¯
        self.conversation_list[wxid].append({"role": role, "content": content})
        
        # åˆ·æ–°æ—¶é—´
        self.conversation_list[wxid][1] = {"role": "system", "content": time_mk}
        
        # æ»šåŠ¨æ¸…é™¤è¶…å‡ºé™åˆ¶çš„åŽ†å²
        if len(self.conversation_list[wxid]) > MAX_CONVERSATION_LENGTH:
            self.LOG.info(f"æ»šåŠ¨æ¸…é™¤èŠå¤©è®°å½•: {wxid}")
            del self.conversation_list[wxid][2]

    @staticmethod
    def _extract_answer(rsp_str: str) -> str:
        """ä»Žå“åº”ä¸­æå– answer å­—æ®µï¼ˆå¦‚æžœæ˜¯ JSON æ ¼å¼ï¼‰"""
        try:
            data = json.loads(rsp_str)
            if isinstance(data, dict) and 'answer' in data:
                return data['answer']
        except json.JSONDecodeError:
            pass
        return rsp_str


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    LOG = logging.getLogger("llm_service")
    
    config = Config().LLM_BOT
    if not config:
        LOG.info("LLM é…ç½®ä¸¢å¤±, æµ‹è¯•è¿è¡Œå¤±è´¥")
        exit(0)
    
    llm = LLMService()
    
    # æµ‹è¯•ç¨‹åº
    while True:
        q = input(">>> ")
        try:
            time_start = datetime.now()
            LOG.info(llm.get_answer(q, "", ""))
            time_end = datetime.now()
            LOG.info(f"{round((time_end - time_start).total_seconds(), 2)}s")
        except Exception as e:
            LOG.error(e)
