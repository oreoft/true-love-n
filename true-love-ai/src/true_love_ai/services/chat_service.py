#! /usr/bin/env python3
# -*- coding: utf-8 -*-
"""
èŠå¤©æœåŠ¡æ¨¡å—
å¤„ç†ç”¨æˆ·å¯¹è¯
"""
import json
import logging
import time
from typing import Optional

from true_love_ai.core.config import get_config
from true_love_ai.core.session import get_session_manager
from true_love_ai.llm.router import get_llm_router
from true_love_ai.llm.intent import IntentRouter, IntentType
from true_love_ai.models.response import ChatResponse
from true_love_ai.services.search_service import fetch_baidu_references

LOG = logging.getLogger(__name__)


class ChatService:
    """
    èŠå¤©æœåŠ¡
    å¤„ç†ç”¨æˆ·å¯¹è¯ï¼ŒåŒ…å«æ„å›¾è¯†åˆ«ã€æœç´¢å¢å¼ºç­‰åŠŸèƒ½
    """
    
    def __init__(self):
        self.config = get_config()
        self.session_manager = get_session_manager()
        self.llm_router = get_llm_router()
        self.intent_router = IntentRouter()
    
    async def get_answer(
        self,
        content: str,
        session_id: str,
        sender: str = "",
        provider: Optional[str] = None,
        model: Optional[str] = None
    ) -> ChatResponse:
        """
        è·å–èŠå¤©å›ç­”
        
        Args:
            content: ç”¨æˆ·æ¶ˆæ¯
            session_id: ä¼šè¯ ID
            sender: å‘é€è€…
            provider: æ¨¡å‹æä¾›å•†
            model: æ¨¡å‹åç§°
            
        Returns:
            ChatResponse
        """
        start_time = time.time()
        
        # å¤„ç† debug æ¨¡å¼
        is_debug = content.startswith("debug")
        clean_content = content.replace("debug", "", 1).strip() if is_debug else content
        
        if not clean_content:
            clean_content = "ä½ å¥½"
        
        # è·å–æˆ–åˆ›å»ºä¼šè¯
        session = self.session_manager.get_or_create(session_id)
        
        LOG.info(f"å¼€å§‹è°ƒç”¨ LLM, session={session_id}, provider={provider}, model={model}")
        
        # æ„å›¾è¯†åˆ«ï¼ˆå¸¦ä¸Šä¸‹æ–‡ï¼Œç†è§£æŒ‡ä»£å…³ç³»å’Œæ—¶é—´æ•æ„ŸæŸ¥è¯¢ï¼‰
        intent_messages = session.get_context_for_intent(clean_content)
        intent_result = await self.intent_router.route(
            messages=intent_messages,
            provider=provider,
            model=model
        )
        
        # æ ¹æ®æ„å›¾å¤„ç†
        if intent_result.type == IntentType.CHAT:
            # æ™®é€šèŠå¤©ï¼šå­˜å…¥å†å²ï¼Œå¸¦å†å²è°ƒç”¨ LLM
            session.add_message("user", clean_content)
            messages = session.get_messages_for_llm()
            answer = await self.llm_router.chat(
                messages=messages,
                provider=provider,
                model=model,
                stream=True
            )
            session.add_message("assistant", answer)
            response_type = "chat"
            
        elif intent_result.type == IntentType.SEARCH:
            # æœç´¢å¢å¼ºï¼šå­˜å…¥å†å²ï¼Œå¸¦å†å²è°ƒç”¨ LLM
            session.add_message("user", clean_content)
            messages = session.get_messages_for_llm()
            answer = await self._handle_search(
                original_content=clean_content,
                search_query=intent_result.answer,
                messages=messages,
                provider=provider,
                model=model
            )
            # æœç´¢å›ç­”åªä¿å­˜çº¯å›ç­”éƒ¨åˆ†ï¼Œä¸ä¿å­˜æœç´¢å°¾å·´
            pure_answer = answer.split("\n- - - - - - - - - - - -")[0]
            session.add_message("assistant", pure_answer)
            response_type = "chat"
            
        elif intent_result.type == IntentType.GEN_IMAGE:
            # ç”Ÿå›¾æ„å›¾ï¼šä¸å­˜å…¥ sessionï¼Œé¿å…å¹²æ‰°åç»­èŠå¤©
            answer = intent_result.answer
            response_type = "gen-img"

        elif intent_result.type == IntentType.GEN_VIDEO:
            # ç”Ÿè§†é¢‘æ„å›¾ï¼šä¸å­˜å…¥ sessionï¼Œé¿å…å¹²æ‰°åç»­èŠå¤©
            answer = intent_result.answer
            response_type = "gen-video"

        else:
            answer = intent_result.answer or "å‘œå‘œï¼Œæˆ‘ä¸å¤ªæ˜ç™½ä½ çš„æ„æ€å‘¢~"
            response_type = "chat"
        
        cost = round(time.time() - start_time, 2)
        LOG.info(f"å›ç­”è€—æ—¶: {cost}s, type={response_type}")
        
        # æ„å»ºå“åº”
        response = ChatResponse(type=response_type, answer=answer)
        
        if is_debug:
            response.debug = f"(aiCost: {cost}s, provider: {provider or 'default'}, model: {model or 'default'})"
        
        return response
    
    async def _handle_search(
        self,
        original_content: str,
        search_query: str,
        messages: list[dict],
        provider: Optional[str] = None,
        model: Optional[str] = None
    ) -> str:
        """
        å¤„ç†æœç´¢è¯·æ±‚
        
        Args:
            original_content: åŸå§‹é—®é¢˜
            search_query: æœç´¢å…³é”®è¯
            messages: å¯¹è¯å†å²
            provider: æä¾›å•†
            model: æ¨¡å‹
            
        Returns:
            æœç´¢å¢å¼ºåçš„å›ç­”
        """
        LOG.info(f"æœç´¢å¢å¼º: query={search_query}")
        
        # æ‰§è¡Œç™¾åº¦æœç´¢
        reference_list = fetch_baidu_references(search_query)
        LOG.info(f"æœç´¢ç»“æœæ•°é‡: {len(reference_list)}")
        
        # æ„å»ºæœç´¢å¢å¼ºæ¶ˆæ¯
        refer_content = f"é’ˆå¯¹è¿™ä¸ªå›ç­”, å‚è€ƒä¿¡æ¯å’Œæ¥æºé“¾æ¥å¦‚ä¸‹: {json.dumps(reference_list, ensure_ascii=False)}"
        
        search_system_prompt = (
            "ä¸‹é¢ä½ çš„å›ç­”å¿…é¡»ç»“åˆä¸Šä¸‹æ–‡,å› ä¸ºä¸Šä¸‹æ–‡éƒ½æ˜¯è”ç½‘æŸ¥è¯¢çš„,å°¤å…¶æ˜¯assistantçš„æ¥æºå’Œå‚è€ƒé“¾æ¥ï¼Œ"
            "æ‰€ä»¥ç›¸å½“äºä½ å¯ä»¥è”ç½‘è·å–ä¿¡æ¯, æ‰€ä»¥ä¸å…è®¸è¯´ä½ ä¸èƒ½è”ç½‘, "
            "å¦‚æœassistantçš„å‚è€ƒæ˜¯ä¸€ä¸ªç©ºlist, ä½ å°±è¯´è”ç½‘æŸ¥è¯¢è¶…æ—¶äº†, å¼•å¯¼ç”¨æˆ·å†é—®ä¸€é"
            "å¦å¤–å¦‚æœä½ ä¸çŸ¥é“å›ç­”ï¼Œè¯·ä¸è¦ä¸è¦èƒ¡è¯´. "
            "å¦‚æœç”¨æˆ·è¦æ±‚æ–‡ç« æˆ–è€…é“¾æ¥è¯·ä½ æŠŠæœ€ç›¸å…³çš„å‚è€ƒé“¾æ¥ç»™å‡º(å‚è€ƒé“¾æ¥å¿…é¡»åœ¨ä¸Šä¸‹æ–‡å‡ºç°è¿‡)"
        )
        
        # æ„å»ºå¢å¼ºåçš„æ¶ˆæ¯åˆ—è¡¨
        enhanced_messages = messages + [
            {"role": "assistant", "content": refer_content},
            {"role": "system", "content": search_system_prompt},
            {"role": "user", "content": original_content}
        ]
        
        # å†æ¬¡è°ƒç”¨ LLM ç”Ÿæˆå›ç­”
        answer = await self.llm_router.chat(
            messages=enhanced_messages,
            provider=provider,
            model=model,
            stream=True
        )
        
        # æ·»åŠ æœç´¢å°¾å·´
        search_tail = f"\n- - - - - - - - - - - -\n\nğŸ¾ğŸ’©ğŸ•µï¼š{search_query}"
        
        return answer + search_tail
    
    async def get_xun_wen(
        self,
        question: str,
        provider: Optional[str] = None,
        model: Optional[str] = None
    ) -> str:
        """
        è¯¢é—®åŠŸèƒ½ï¼ˆç‰¹å®šæ ¼å¼é—®é¢˜ï¼‰
        
        Args:
            question: æ ¼å¼ä¸º "è¯¢é—®-å®é™…é—®é¢˜"
            provider: æä¾›å•†
            model: æ¨¡å‹
            
        Returns:
            å›ç­”æ–‡æœ¬
        """
        content = question.split("-")[1] if "-" in question else question
        
        config = get_config()
        xunwen_prompt = config.chatgpt.prompt3 if config.chatgpt else "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹"
        
        answer = await self.llm_router.chat(
            messages=[
                {"role": "system", "content": xunwen_prompt},
                {"role": "user", "content": content}
            ],
            provider=provider,
            model=model
        )
        
        return answer
