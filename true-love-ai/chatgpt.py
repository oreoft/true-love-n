#! /usr/bin/env python3
# -*- coding: utf-8 -*-
import base64
import json
import logging
import subprocess
import time
from datetime import datetime
from io import BytesIO
from urllib.parse import quote_plus

import requests
from litellm import Router

from configuration import Config

name = "chatgpt"
openai_vision_model = "gpt-4o"
openai_model = "gpt-4o"
claude_model = "claude-3-5-sonnet-20241022"
baidu_curl = ("curl --location 'https://www.baidu.com/s?wd=%s&tn=json' "
              "--header 'User-Agent: Mozilla/5.0 (iPhone; CPU iPhone OS 16_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Mobile/15E148 Safari/604.1'")
sd_url = "https://api.stability.ai/v2beta/stable-image/generate/ultra"
sd_gen_url = "https://api.stability.ai/v2beta/stable-image/control/structure"
sd_erase_url = "https://api.stability.ai/v2beta/stable-image/edit/erase"
sd_replace_url = "https://api.stability.ai/v2beta/stable-image/edit/search-and-replace"
sd_remove_background_url = "https://api.stability.ai/v2beta/stable-image/edit/remove-background"
sd_url_map = {'gen_by_img': sd_gen_url,
              'erase_img': sd_erase_url,
              'replace_img': sd_replace_url,
              'remove_background_img': sd_remove_background_url
              }

type_answer_call = [
    {
        "type": "function",
        "function": {
            "name": "type_answer",
            "description": "type_answer",
            "parameters": {
                "type": "object",
                "properties": {
                    "type": {
                        "type": "string",
                        "description": "the type of question, "
                                       "if user wants you to generate images, please return the 'gen-img', "
                                       "if it is a normal chat, please return the 'chat', "
                                       "if the content requires online search You search in context first "
                                       "and if there is no information, please return the 'search'"
                    },
                    "answer": {
                        "type": "string",
                        "description": "the answer of content, "
                                       "if type is 'chat', please put your answer in this field, "
                                       "if type is 'gen-img', "
                                       "Please combine the context to give the descriptive words needed to generate the image."
                                       "if type is 'search', è¯·åœ¨æ­¤å­—æ®µä¸­è¿”å›è¦æœç´¢çš„å†…å®¹å…³é”®è¯, å¿…é¡»æ˜¯ä¸­æ–‡, "
                                       "å¦‚æœå…¶ä»–ç±»å‹, This can be empty, "
                    },
                },
                "required": [
                    "type",
                    "answer"
                ]
            }
        }
    }
]

img_type_answer_call = [
    {
        "type": "function",
        "function": {
            "name": "img_type_answer_call",
            "description": "img_type_answer_call",
            "parameters": {
                "type": "object",
                "properties": {
                    "type": {
                        "type": "string",
                        "description": "Based on the user description, determine which of the following types it belongs to: "
                                       "generate image (gen_by_img), "
                                       "erase object from image (erase_img), "
                                       "replace object in image (replace_img), "
                                       "analyzing or interpreting image (analyze_img), "
                                       "remove image background (remove_background_img). "
                                       "Please provide the type."
                    },
                    "answer": {
                        "type": "string",
                        "description": "Here is your translation and colorization answer, please put your answer in this field"
                    },
                },
                "required": [
                    "type",
                    "answer"
                ]
            }
        }
    }
]


def fetch_stream(ret, is_f=False):
    rsp = ''
    for stream_res in ret:
        try:
            if is_f:
                # å¤„ç†å‡½æ•°/å·¥å…·è°ƒç”¨
                if stream_res.choices[0].delta.tool_calls:
                    tool_call = stream_res.choices[0].delta.tool_calls[0]
                    if tool_call.function.arguments:
                        rsp += tool_call.function.arguments.replace('\n\n', '\n')
            else:
                # å¤„ç†æ™®é€šæ–‡æœ¬å†…å®¹
                if stream_res.choices[0].delta.content:
                    rsp += stream_res.choices[0].delta.content.replace('\n\n', '\n')
        except Exception as e:
            print(f"Error processing stream response: {e}")
            continue

    return rsp


class ChatGPT:

    def __init__(self) -> None:
        self.LOG = logging.getLogger("ChatGPT")
        self.config = Config().LLM_BOT
        self.router = Router(model_list=[
            {
                "model_name": openai_model,
                "litellm_params": {
                    "model": openai_model,
                    "api_key": self.config.get('key1')
                }
            },
            {
                "model_name": openai_model,
                "litellm_params": {
                    "model": openai_model,
                    "api_key": self.config.get('key2')
                }
            },
            {
                "model_name": openai_model,
                "litellm_params": {
                    "model": openai_model,
                    "api_key": self.config.get('key3')
                }
            },
            {
                "model_name": claude_model,
                "litellm_params": {
                    "model": claude_model,
                    "api_key": self.config.get('claude_key1')
                }
            }
        ])
        # å¯¹è¯å†å²å®¹å™¨
        self.conversation_list = {}
        # æç¤ºè¯åŠ è½½
        self.system_content_msg = {"role": "system", "content": self.config.get("prompt")}
        self.system_content_msg2 = {"role": "system", "content": self.config.get("prompt2")}
        self.system_content_msg3 = {"role": "system", "content": self.config.get("prompt3")}
        self.system_content_msg4 = {"role": "system", "content": self.config.get("prompt4")}
        self.system_content_msg5 = {"role": "system", "content": self.config.get("prompt5")}
        self.system_content_msg6 = {"role": "system", "content": self.config.get("prompt6")}

    def get_xun_wen(self, question):
        content = question.split("-")[1]
        return self.send_gpt_by_message([self.system_content_msg3, {"role": "user", "content": content}])

    def send_gpt_by_message(self, messages, function_call=None, functions=None):
        try:
            # å‘é€è¯·æ±‚
            ret = self.router.completion(
                model=openai_model,
                messages=messages,
                temperature=0.2,
                tool_choice=function_call,
                tools=functions,
                stream=True
            )
            # è·å–streamæŸ¥è¯¢
            rsp = fetch_stream(ret, functions)
        except Exception as e0:
            rsp = "An unknown error has occurred. Try again later."
            self.LOG.error(str(e0))
        return rsp

    def send_chatgpt(self, real_model, wxid) -> dict:
        try:
            # å‘é€è¯·æ±‚
            question = self.conversation_list[wxid][-1]
            ret = self.router.completion(
                model=real_model,
                messages=self.conversation_list[wxid],
                temperature=0.2,
                tool_choice={"type": "function", "function": {"name": "type_answer"}},
                tools=type_answer_call,
                stream=True
            )
            # è·å–streamæŸ¥è¯¢
            rsp_str = fetch_stream(ret, True)
            result = json.loads(rsp_str)
            rsp = result
            self.LOG.info(f"openai result :{result}")
            if result['type'] == 'search':
                # å…ˆå»ç™¾åº¦è·å–æ•°æ®
                reference_list = self.fetch_refer_baidu(result)
                logging.info(f"fetch_refer_baidu, result one:{reference_list[0] if reference_list else {} }")
                # æ„å»ºä¸´æ—¶prompt
                refer_prompt = {"role": "assistant",
                                "content": f"é’ˆå¯¹è¿™ä¸ªå›ç­”, å‚è€ƒä¿¡æ¯å’Œæ¥æºé“¾æ¥å¦‚ä¸‹: {json.dumps(reference_list)}"}
                temp_prompt = {"role": "system",
                               "content": "ä¸‹é¢ä½ çš„å›ç­”å¿…é¡»ç»“åˆä¸Šä¸‹æ–‡,å› ä¸ºä¸Šä¸‹æ–‡éƒ½æ˜¯è”ç½‘æŸ¥è¯¢çš„,å°¤å…¶æ˜¯assistantçš„æ¥æºå’Œå‚è€ƒé“¾æ¥ï¼Œ"
                                          "æ‰€ä»¥ç›¸å½“äºä½ å¯ä»¥è”ç½‘è·å–ä¿¡æ¯, æ‰€ä»¥ä¸å…è®¸è¯´ä½ ä¸èƒ½è”ç½‘, "
                                          "å¦‚æœassistantçš„å‚è€ƒæ˜¯ä¸€ä¸ªç©ºlist, ä½ å°±è¯´è”ç½‘æŸ¥è¯¢è¶…æ—¶äº†, å¼•å¯¼ç”¨æˆ·å†é—®ä¸€é"
                                          "å¦å¤–å¦‚æœä½ ä¸çŸ¥é“å›ç­”ï¼Œè¯·ä¸è¦ä¸è¦èƒ¡è¯´. "
                                          "å¦‚æœç”¨æˆ·è¦æ±‚æ–‡ç« æˆ–è€…é“¾æ¥è¯·ä½ æŠŠæœ€ç›¸å…³çš„å‚è€ƒé“¾æ¥ç»™å‡º(å‚è€ƒé“¾æ¥å¿…é¡»åœ¨ä¸Šä¸‹æ–‡å‡ºç°è¿‡)"}
                # ç„¶åå†æ‹¿ç»“æœå»é—®chatgpt
                ret = self.router.completion(
                    model=real_model,
                    messages=self.conversation_list[wxid] + [refer_prompt, temp_prompt, question],
                    temperature=0.2,
                    stream=True
                )
                # è·å–streamæŸ¥è¯¢
                rsp_str = fetch_stream(ret)
                search_tail = f"\n- - - - - - - - - - - -\n\nğŸ¾ğŸ’©ğŸ•µï¼š{result['answer']}"
                rsp = {"type": "chat", "answer": rsp_str + search_tail}
                self.LOG.info(f"openai+baidu:{rsp}")
            self._update_message(wxid, rsp_str, "assistant")
        except Exception as e0:
            rsp = {"type": "chat", "answer": "å‘ç”ŸæœªçŸ¥é”™è¯¯, ç¨åå†è¯•è¯•æ"}
            self.LOG.exception('è°ƒç”¨åŒ—ç¾aiæœåŠ¡å‘ç”Ÿé”™è¯¯, msg: %s', e0)
        return rsp

    def fetch_refer_baidu(self, result):
        reference_list = []
        try:
            send_curl = baidu_curl % quote_plus(result['answer'])
            self.LOG.info(f"need go to baidu search: {result['answer']}, curl:{send_curl}")
            baidu_response = subprocess.run(send_curl, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE,
                                            text=True)
            # è·å–å‘½ä»¤è¾“å‡º
            # ä½¿ç”¨json.loadsè§£æå“åº”ä½“
            data = json.loads(baidu_response.stdout)
            # ä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼ä»æ¯ä¸ªentryä¸­æå–å­—æ®µçš„å€¼
            reference_list = [
                {"content": entry['abs'], "source_url": entry['url']}
                for entry in data['feed']['entry']
                if 'abs' in entry and 'url' in entry
            ]
        except Exception:
            logging.exception(f"fetch_refer_baidu error, result:{result}")
        return reference_list

    def get_answer(self, question: str, wxid: str, sender: str) -> dict:
        self._update_message(wxid, question.replace("debug", "", 1) if question else 'ä½ å¥½', "user")
        start_time = time.time()
        self.LOG.info("å¼€å§‹å‘é€ç»™chatgptï¼Œ å…¶ä¸­real_model: %s", openai_model)
        rsp = self.send_chatgpt(openai_model, wxid)
        end_time = time.time()
        cost = round(end_time - start_time, 2)
        self.LOG.info("chatå›ç­”æ—¶é—´ä¸ºï¼š%s ç§’", cost)
        if question.startswith('debug'):
            rsp[
                'debug'] = f"(aiCost: {cost}s, ioCost: $s, model: {openai_model})"
        return rsp

    def _update_message(self, wxid: str, aq: str, role: str) -> None:
        time_mk = f"å½“éœ€è¦å›ç­”å½“å‰æ—¶é—´æˆ–è€…å…³äºå½“å‰æ—¥æœŸç±»é—®é¢˜, è¯·ç›´æ¥å‚è€ƒè¿™ä¸ªæ—¶é—´: {str(datetime.now().strftime('%Y-%m-%d %H:%M:%S'))}(è¯·æ³¨æ„è¿™æ˜¯ç¾å›½ä¸­éƒ¨æ—¶é—´, ä½ å¯ä»¥å‘Šè¯‰åˆ«äººä½ ä½¿ç”¨çš„æ—¶åŒº), å¦å¤–ç”¨æˆ·æå‡æ˜¯å¦å¯ä»¥è”ç½‘ä½ éœ€è¦è¯´æˆ‘å·²ç»æ¥å…¥è°·æ­Œæœç´¢, å¹¶ä¸”çŸ¥è¯†åº“æœ€æ–°æ¶ˆæ¯æ˜¯: {str(datetime.now().strftime('%Y-%m-%d %H:%M:%S'))}"
        # åˆå§‹åŒ–èŠå¤©è®°å½•,ç»„è£…ç³»ç»Ÿä¿¡æ¯
        if wxid not in self.conversation_list.keys():
            self.conversation_list[wxid] = [
                self.system_content_msg if wxid not in self.config.get("gpt4") else self.system_content_msg2,
                {"role": "system", "content": time_mk}
            ]

        # å½“å‰é—®é¢˜
        content_question_ = {"role": role, "content": aq}
        self.conversation_list[wxid].append(content_question_)

        # åˆ·æ–°å½“å‰æ—¶é—´
        self.conversation_list[wxid][1] = {"role": "system", "content": time_mk}
        # åªå­˜å‚¨10æ¡è®°å½•ï¼Œè¶…è¿‡æ»šåŠ¨æ¸…é™¤
        if len(self.conversation_list[wxid]) > 10:
            self.LOG.info("æ»šåŠ¨æ¸…é™¤èŠå¤©è®°å½•ï¼š%s", wxid)
            # åˆ é™¤å¤šä½™çš„è®°å½•ï¼Œå€’ç€åˆ ï¼Œä¸”è·³è¿‡ç¬¬äºŒä¸ªçš„ç³»ç»Ÿæ¶ˆæ¯
            del self.conversation_list[wxid][2]

    def get_analyze_by_img(self, content, img_data, wxid):
        self._update_message(wxid, content.replace("debug", "", 1), "user")
        try:
            start_time = time.time()
            self.LOG.info("get_analyze_by_img start")
            ret = self.router.completion(
                model=openai_vision_model,
                messages=[
                    self.system_content_msg6,
                    {"role": "user", "content": [
                        {"type": "text", "text": content},
                        {"type": "image_url", "image_url": {
                            "url": f"data:image/png;base64,{img_data}"}
                         }
                    ]}
                ],
                temperature=0.2,
                stream=True
            )
            cost = round(time.time() - start_time, 2)
            self.LOG.info(f"get_analyze_by_img cost:[{cost}ms]")
            # è·å–streamæŸ¥è¯¢
            result = fetch_stream(ret)
            # æ›´æ–°è¿”å›å€¼
            self._update_message(wxid, result, "assistant")
            if content.startswith('debug'):
                result = result + '\n\n' + f"aiCost: {cost}s, use: {'12123'[-4:]}, model: {openai_model})"
            return result
        except requests.Timeout:
            self.LOG.error(f"get_analyze_by_img timeout")
            raise
        except Exception as e:
            self.LOG.exception(f"get_analyze_by_img error")
            raise

    def get_img_type(self, content):
        try:
            start_time = time.time()
            self.LOG.info("ds.img.typeAndPrompt start")
            image_prompt = self.send_gpt_by_message(
                messages=[
                    self.system_content_msg5,
                    {"role": "user", "content": content}
                ],
                function_call={"type": "function", "function": {"name": "img_type_answer_call"}},
                functions=img_type_answer_call,
            )
            self.LOG.info(f"ds.typeAndPrompt cost:[{(time.time() - start_time) * 1000}ms] result:{image_prompt}")
            return image_prompt
        except Exception:
            self.LOG.exception(f"generate_typeAndPrompt error")

    def get_img_by_img(self, content, img_data):
        # First get the image prompt
        image_prompt = content

        # Re-generate the image based on the prompt
        try:
            start_time = time.time()
            self.LOG.info("ds.img start")
            response = requests.post(
                sd_url_map.get(image_prompt["type"], sd_url),
                headers={
                    "authorization": f"Bearer {Config().PLATFORM_KEY['sd']}",
                    "accept": "application/json; type=image/"
                },
                files={
                    "image": BytesIO(base64.b64decode(img_data))
                },
                data={
                    "prompt": image_prompt["answer"],
                    "search_prompt": image_prompt["answer"],
                    "control_strength": 0.7,
                    "output_format": "png"
                },
            )
            self.LOG.info(f"ds.img cost:[{(time.time() - start_time) * 1000}ms]")
            if response.status_code == 200:
                return {"prompt": image_prompt["answer"], "img": response.json()['image']}
            else:
                self.LOG.error(f"generate_image_with_sd not 200, result:{response.json()}")
                raise ValueError("ç”Ÿæˆå¤±è´¥! å†…å®¹å¤ªä¸å ªå…¥ç›®å•¦~")
        except requests.Timeout:
            self.LOG.error(f"generate_image_with_sd timeout")
            raise
        except Exception:
            self.LOG.exception(f"generate_image_with_sd error")
            raise

    def get_img(self, content):
        # First get the image prompt
        image_prompt = ""
        try:
            start_time = time.time()
            self.LOG.info("ds.img.prompt start")
            image_prompt = self.send_gpt_by_message(messages=[
                self.system_content_msg4,
                {"role": "user", "content": content}
            ])
            self.LOG.info(f"ds.prompt cost:[{(time.time() - start_time) * 1000}ms]")
        except Exception:
            self.LOG.exception(f"generate_prompt error")

        # Re-generate the image based on the prompt
        try:
            start_time = time.time()
            self.LOG.info("ds.img start")
            response = requests.post(sd_url,
                                     headers={
                                         "authorization": f"Bearer {Config().PLATFORM_KEY['sd']}",
                                         "accept": "application/json; type=image/"
                                     },
                                     files={"none": ''},
                                     data={
                                         "prompt": image_prompt,
                                         "output_format": "jpeg",
                                         "aspect_ratio": "1:1"
                                     },
                                     )
            self.LOG.info(f"ds.img cost:[{(time.time() - start_time) * 1000}ms]")
            if response.status_code == 200:
                return {"prompt": image_prompt, "img": response.json()['image']}
            else:
                self.LOG.error(f"generate_image_with_sd not 200, result:{response.json()}")
                raise ValueError("ç”Ÿæˆå¤±è´¥! å†…å®¹å¤ªä¸å ªå…¥ç›®å•¦~")
        except requests.Timeout:
            self.LOG.error(f"generate_image_with_sd timeout")
            raise
        except Exception:
            self.LOG.exception(f"generate_image_with_sd error")
            raise


if __name__ == "__main__":
    LOG = logging.getLogger("chatgpt")
    config: dict = Config().LLM_BOT
    if not config:
        LOG.info("chatgpté…ç½®ä¸¢å¤±, æµ‹è¯•è¿è¡Œå¤±è´¥")
        exit(0)
    chat = ChatGPT()
    # æµ‹è¯•ç¨‹åº
    while True:
        q = input(">>> ")
        try:
            time_start = datetime.now()  # è®°å½•å¼€å§‹æ—¶é—´
            LOG.info(chat.get_answer(q, "", ""))
            time_end = datetime.now()  # è®°å½•ç»“æŸæ—¶é—´
            LOG.info(f"{round((time_end - time_start).total_seconds(), 2)}s")  # è®¡ç®—çš„æ—¶é—´å·®ä¸ºç¨‹åºçš„æ‰§è¡Œæ—¶é—´ï¼Œå•ä½ä¸ºç§’/s
        except Exception as e:
            LOG.error(e)
